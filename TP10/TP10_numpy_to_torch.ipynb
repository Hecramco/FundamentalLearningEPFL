{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Numpy to PyTorch\n",
    "\n",
    "**What will you learn in this notebook**:\n",
    "- Tensors syntax\n",
    "- Autograd\n",
    "- Neural Network modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get back to last week exercise and migrate it to PyTorch. Luckily, the syntax is almost identical. The main difference is that *arrays* are replaced by *tensors*, and all the `np.*` functions become `torch.*`. For more advanced functionalities, we refer you to the [official documentation][torch_doc].\n",
    "\n",
    "[torch_doc]: https://pytorch.org/docs/stable/index.html\n",
    "\n",
    "## Single layer MLP in Numpy\n",
    "\n",
    "Recall the feedforward neural network with a single hidden layer.\n",
    "\n",
    "![simple_mlp](./simple_mlp.png)\n",
    "\n",
    "Below is the Numpy implementation of the activations and the feedforward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "def np_sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1.0 / (1 + np.exp(-t))\n",
    "\n",
    "def np_grad_sigmoid(t):\n",
    "    \"\"\"return the derivative of sigmoid on t.\"\"\"\n",
    "    return np_sigmoid(t) * (1 - np_sigmoid(t))\n",
    "\n",
    "def np_mlp(\n",
    "    x: NDArray[np.float_], w_1: NDArray[np.float_], w_2: NDArray[np.float_]\n",
    ") -> Tuple[NDArray[np.float_], NDArray[np.float_], NDArray[np.float_]]:\n",
    "    \"\"\"Feed forward propagation on MLP\n",
    "\n",
    "    Args:\n",
    "        x (NDArray[np.float_]): Input vector of shape (d_in,)\n",
    "        w_1 (NDArray[np.float_]): Parameter matrix of first hidden layer, of shape (d_in, d_hid)\n",
    "        w_2 (NDArray[np.float_]): Parameter vector of output layer, of shape (d_hid,)\n",
    "\n",
    "    Returns:\n",
    "        Tuple[NDArray[np.float], NDArray[np.float], NDArray[np.float]]: Three\n",
    "            arrays `y_hat`, `z_1`, `z_2`, containing repsectively the output and\n",
    "            the two preactivations.\n",
    "    \"\"\"\n",
    "    z_1 = w_1.T @ x\n",
    "    x_1 = np_sigmoid(z_1)\n",
    "    z_2 = w_2.T @ x_1\n",
    "    y_hat = np_sigmoid(z_2)\n",
    "    \n",
    "    return y_hat, z_1, z_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the backpropagation with the Mean-squared error loss $\\mathcal L (y, \\hat y) = \\frac{1}{2} \\left( y - \\hat y \\right)^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_mlp_backpropagation(\n",
    "    y: NDArray[np.int_],\n",
    "    x: NDArray[np.float_],\n",
    "    w_2: NDArray[np.float_],\n",
    "    y_hat: NDArray[np.float_],\n",
    "    z_1: NDArray[np.float_],\n",
    "    z_2: NDArray[np.float_],\n",
    ") -> Tuple[NDArray[np.float_], NDArray[np.float_]]:\n",
    "    \"\"\"Do backpropagation and get parameter gradients.\n",
    "\n",
    "    Args:\n",
    "        y (NDArray[np.int_]): True label\n",
    "        x (NDArray[np.float_]): Input data\n",
    "        w_2 (NDArray[np.float_]): Readout layer parameters\n",
    "        y_hat (NDArray[np.float_]): MLP output\n",
    "        z_1 (NDArray[np.float_]): Hidden layer preactivations\n",
    "        z_2 (NDArray[np.float_]): Readout layer preactivations\n",
    "\n",
    "    Returns:\n",
    "        Tuple[NDArray[np.float_], NDArray[np.float_]]: Gradients of w_1 and w_2\n",
    "    \"\"\"\n",
    "    # Feed forward\n",
    "    _loss = 0.5 * (y - y_hat)**2\n",
    "\n",
    "    # Backpropogation\n",
    "    delta_2 = (y_hat - y) * np_grad_sigmoid(z_2)\n",
    "    x_1 = np_sigmoid(z_1)\n",
    "    dw_2 = delta_2 * x_1\n",
    "    delta_1 = delta_2 * w_2* np_grad_sigmoid(z_1)\n",
    "    dw_1 = np.outer(x, delta_1)\n",
    "\n",
    "    return dw_1, dw_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the MLP output and retrieve the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "x_np = np.array([0.01, 0.02, 0.03, 0.04])\n",
    "w_1_np = np.random.randn(4, 5)\n",
    "w_2_np = np.random.randn(5)\n",
    "\n",
    "y = 1\n",
    "\n",
    "y_hat_np, z_1, z_2 = np_mlp(x_np, w_1_np, w_2_np)\n",
    "dw_1_np, dw_2_np = np_mlp_backpropagation(y, x_np, w_2_np, y_hat_np, z_1, z_2)\n",
    "\n",
    "print(dw_1_np.shape)\n",
    "print(dw_2_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indeed works, but as soon as we change the neural network architecture we have to change our backpropagation function, and keep track of all the computations that involve each parameter. It is a lot of work which we want to delegate to the machine.\n",
    "This is what *automatic differentiation* does, and libraries like PyTorch implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "We can manipulate tensors as we want and, by asking for `require_grad=True`, PyTorch handles automatic differentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c tensor(nan, grad_fn=<SumBackward0>)\n",
      "c.shape: torch.Size([])\n",
      "\n",
      "b.grad: tensor([-2.5950,  7.9772,  0.9260,  0.2634,  3.4284])\n",
      "b.grad.shape: torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE\n",
    "\n",
    "a = torch.randn(10, 5)\n",
    "b = torch.ones(5, requires_grad=True)\n",
    "\n",
    "# Note that c is a scalar\n",
    "c = torch.log(a @ b).sum()\n",
    "print(\"c\", c)\n",
    "print(\"c.shape:\", c.shape)\n",
    "print()\n",
    "\n",
    "# We ask to perform backpropagation\n",
    "c.backward()\n",
    "\n",
    "\n",
    "print(\"b.grad:\", b.grad)\n",
    "print(\"b.grad.shape:\", b.grad.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now convert the previous code to PyTorch. Autograd is responsible of keeping track of each element in the computations, so we only need to implement the forward pass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t) -> torch.FloatTensor:\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    #vvvvv YOUR CODE HERE vvvvv#\n",
    "    sig = 1.0 / (1 + torch.exp(-t))\n",
    "    return sig\n",
    "    #^^^^^^^^^^^^^^^^^^^^^^^^^^#\n",
    "\n",
    "def mlp(\n",
    "    x: torch.Tensor, w_1: torch.Tensor, w_2: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Feed forward propagation on MLP\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input vector of shape (d_in,)\n",
    "        w_1 (torch.Tensor): Parameter matrix of first hidden layer, of shape (d_in, d_hid)\n",
    "        w_2 (torch.Tensor): Parameter vector of output layer, of shape (d_hid,)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Network output\n",
    "    \"\"\"\n",
    "    #vvvvv YOUR CODE HERE vvvvv#\n",
    "    z_1 = w_1.T @ x\n",
    "    x_1 = sigmoid(z_1)\n",
    "    z_2 = w_2.T @ x_1\n",
    "    y_hat = sigmoid(z_2)\n",
    "    #^^^^^^^^^^^^^^^^^^^^^^^^^^#\n",
    "    \n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can verify that the output corresponds to the numpy implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#vvvvv YOUR CODE HERE vvvvv#\n",
    "\n",
    "# Convert arrays to tensors. Mind that we will ask for parameters gradients!\n",
    "x = torch.tensor(x_np)\n",
    "w_1 = torch.tensor(w_1_np, requires_grad=True)\n",
    "w_2 = torch.tensor(w_2_np, requires_grad=True)\n",
    "\n",
    "\n",
    "y_hat = mlp(x, w_1, w_2)\n",
    "loss = 0.5 * (y - y_hat)**2\n",
    "\n",
    "#Now perform backpropagation\n",
    "loss.backward()\n",
    "\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^#\n",
    "\n",
    "print(np.allclose(w_1.grad.numpy(), dw_1_np))\n",
    "print(np.allclose(w_2.grad.numpy(), dw_2_np))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Computing gradients has now got much easier! :grin:\n",
    "\n",
    "Still, PyTorch provides an even easier interface to build and train neural networks, whose components are in the `torch.nn` module.\n",
    "The main tool is the `torch.nn.Module` class, from which all neural networks shall inherit. This must implement a `forward` method, and, if needed, declare its parameters in the `__init__` method. \n",
    "\n",
    "Let's convert our MLP to a proper Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_hidden: int) -> None:\n",
    "        #vvvvv YOUR CODE HERE vvvvv#\n",
    "        super().__init__()\n",
    "\n",
    "        self.w_1 = torch.nn.Parameter(\n",
    "            torch.randn(dim_in, dim_hidden, requires_grad=True),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.w_2 = torch.nn.Parameter(\n",
    "            torch.randn(dim_hidden, requires_grad=True),\n",
    "            requires_grad= True,\n",
    "        )\n",
    "        #^^^^^^^^^^^^^^^^^^^^^^^^^^#\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #vvvvv YOUR CODE HERE vvvvv#\n",
    "        z_1 = self.w_1.T @ x\n",
    "        x_1 = sigmoid(z_1)\n",
    "        z_2 = self.w_2.T @ x_1\n",
    "        y_hat = sigmoid(z_2)\n",
    "        return y_hat\n",
    "        #^^^^^^^^^^^^^^^^^^^^^^^^^^#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, `torch.nn` comes with a lot of layers and functions which are ready to use.\n",
    "\n",
    "For instance, we have a `torch.sigmoid` function, as well as `torch.nn.Linear` layer and a `torch.nn.MSELoss` loss.\n",
    "\n",
    "Complete this minimal implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MyMLP(nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_hidden: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # NOTE: Linear has a `bias` term by default!\n",
    "        #vvvvv YOUR CODE HERE vvvvv#\n",
    "        self.linear1 = nn.Linear(dim_in, dim_hidden, bias=False)\n",
    "        self.linear2 = nn.Linear(dim_hidden, 1, bias=False)\n",
    "\n",
    "        #^^^^^^^^^^^^^^^^^^^^^^^^^^#\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x).sigmoid()\n",
    "        return self.linear2(x).sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now initialize your model and compute the gradients with resect to the MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyMLP(\n",
      "  (linear1): Linear(in_features=5, out_features=10, bias=False)\n",
      "  (linear2): Linear(in_features=10, out_features=1, bias=False)\n",
      ")\n",
      "<generator object Module.parameters at 0x0000016967141820>\n"
     ]
    }
   ],
   "source": [
    "DIM_IN = 5\n",
    "DIM_HIDDEN = 10\n",
    "\n",
    "x = torch.ones(DIM_IN)\n",
    "y = torch.tensor([0.1])\n",
    "\n",
    "#vvvvv YOUR CODE HERE vvvvv#\n",
    "\n",
    "my_mlp = MyMLP(DIM_IN, DIM_HIDDEN)\n",
    "print(my_mlp)\n",
    "\n",
    "MSE = torch.nn.MSELoss()\n",
    "loss = MSE(my_mlp(x), y)\n",
    "\n",
    "# Backpropagate\n",
    "loss.backward()\n",
    "\n",
    "print(my_mlp.parameters())\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Check the sizes of the gradients of each layer and verify that they correspond to what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0189e-03,  2.0189e-03,  2.0189e-03,  2.0189e-03,  2.0189e-03],\n",
      "        [ 1.0809e-02,  1.0809e-02,  1.0809e-02,  1.0809e-02,  1.0809e-02],\n",
      "        [ 1.1068e-02,  1.1068e-02,  1.1068e-02,  1.1068e-02,  1.1068e-02],\n",
      "        [-1.1595e-02, -1.1595e-02, -1.1595e-02, -1.1595e-02, -1.1595e-02],\n",
      "        [-2.1698e-03, -2.1698e-03, -2.1698e-03, -2.1698e-03, -2.1698e-03],\n",
      "        [-7.7829e-04, -7.7829e-04, -7.7829e-04, -7.7829e-04, -7.7829e-04],\n",
      "        [ 3.5415e-03,  3.5415e-03,  3.5415e-03,  3.5415e-03,  3.5415e-03],\n",
      "        [-4.1721e-03, -4.1721e-03, -4.1721e-03, -4.1721e-03, -4.1721e-03],\n",
      "        [-4.9388e-03, -4.9388e-03, -4.9388e-03, -4.9388e-03, -4.9388e-03],\n",
      "        [-2.3954e-05, -2.3954e-05, -2.3954e-05, -2.3954e-05, -2.3954e-05]])\n",
      "tensor([[0.0433, 0.0799, 0.0570, 0.0604, 0.1405, 0.1607, 0.0977, 0.0548, 0.1152,\n",
      "         0.1170]])\n"
     ]
    }
   ],
   "source": [
    "#vvvvv YOUR CODE HERE vvvvv#\n",
    "gradient_1 = my_mlp.linear1.weight.grad\n",
    "print(gradient_1)\n",
    "gradient_2 = my_mlp.linear2.weight.grad\n",
    "print(gradient_2)\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nn.Sequential` module stacks the given layer one after the other.\n",
    "Still, to get more control on the forward, is better to stick to self-defined module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=5, out_features=10, bias=False)\n",
      "  (1): Sigmoid()\n",
      "  (2): Linear(in_features=10, out_features=1, bias=False)\n",
      "  (3): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "sequential_mlp = nn.Sequential(\n",
    "    nn.Linear(DIM_IN, DIM_HIDDEN, bias=False),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(DIM_HIDDEN, 1, bias=False),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "print(sequential_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "02f5945d149b04c31ea982f93ee137d9e8cdf50633c2456cfcab8268279ed0d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
